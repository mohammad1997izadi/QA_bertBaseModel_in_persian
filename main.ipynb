{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b053bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218c48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/'\n",
    "\n",
    "train_path = path + 'QA_train.json'\n",
    "\n",
    "eval_path = path + 'QA_test.json'\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "\n",
    "\n",
    "configuration = BertConfig()  # default parameters and configuration for BERT\n",
    "\n",
    "# Save the slow pretrained tokenizer\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",max_length=max_len)\n",
    "save_path = \"bert_base_uncased/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "slow_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True, clean_text=True, strip_accents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuations\n",
    "    exclude = set(string.punctuation)\n",
    "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Remove articles\n",
    "    regex = re.compile(r\"\\b(و|با|در)\\b\", re.UNICODE)\n",
    "    text = re.sub(regex, \" \", text)\n",
    "\n",
    "    # Remove extra white space\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "class ExactMatch(keras.callbacks.Callback):\n",
    "  \n",
    "    def __init__(self, x_eval, y_eval):\n",
    "      \n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "       \n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "\n",
    "            normalized_pred_ans = normalize_text(pred_ans)\n",
    "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            \n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941b6016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n",
      "4479\n"
     ]
    }
   ],
   "source": [
    "class TokenizedData:\n",
    "    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "\n",
    "    def preprocessing(self):\n",
    "        context = self.context\n",
    "        question = self.question\n",
    "        answer_text = self.answer_text\n",
    "        start_char_idx = self.start_char_idx\n",
    "\n",
    "\n",
    "        # Clean context, answer and question\n",
    "        context = \" \".join(str(context).split())\n",
    "        question = \" \".join(str(question).split())\n",
    "        answer = \" \".join(str(answer_text).split())\n",
    "\n",
    "        # Find end character index of answer in context\n",
    "        end_char_idx = start_char_idx + len(answer)\n",
    "        if end_char_idx >= len(context):\n",
    "            self.skip = True\n",
    "            return\n",
    "        context = context[0:end_char_idx]\n",
    "        # Mark the character indexes in context that are in answer\n",
    "        is_char_in_ans = [0] * len(context)\n",
    "        for idx in range(start_char_idx, end_char_idx):\n",
    "            is_char_in_ans[idx] = 1\n",
    "\n",
    "        # Tokenize context\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "\n",
    "        # Find tokens that were created from answer characters\n",
    "        ans_token_idx = []\n",
    "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "            if sum(is_char_in_ans[start:end]) > 0:\n",
    "                ans_token_idx.append(idx)\n",
    "\n",
    "        if len(ans_token_idx) == 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        # Find start and end token index for tokens from answer\n",
    "        start_token_idx = ans_token_idx[0]\n",
    "        end_token_idx = ans_token_idx[-1]\n",
    "\n",
    "        # Tokenize question\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "\n",
    "        # Create inputs\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
    "            tokenized_question.ids[1:]\n",
    "        )\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Pad and create attention masks.\n",
    "        # Skip if truncation is needed\n",
    "        padding_length = max_len - len(input_ids)\n",
    "\n",
    "        if padding_length > 0:  # pad\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:  # skip\n",
    "\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.start_token_idx = start_token_idx\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.context_token_to_char = tokenized_context.offsets\n",
    "\n",
    "\n",
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "\n",
    "def data_parser(raw_data):\n",
    "    squad_examples = []\n",
    "    for item in raw_data[\"data\"]:\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if qa['is_impossible']:\n",
    "                    continue\n",
    "                question = qa[\"question\"]\n",
    "               \n",
    "                answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
    "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "                end_char_idx = qa[\"answers\"][0][\"answer_end\"]\n",
    "                squad_eg = TokenizedData(\n",
    "                    question, context, start_char_idx, answer_text, all_answers\n",
    "                )\n",
    "                squad_eg.preprocessing()\n",
    "                squad_examples.append(squad_eg)\n",
    "    return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                \n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "  \n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_squad_examples = data_parser(raw_train_data)\n",
    "x_train, y_train = create_inputs(train_squad_examples)\n",
    "\n",
    "\n",
    "eval_squad_examples = data_parser(raw_eval_data)\n",
    "x_eval, y_eval = create_inputs(eval_squad_examples)\n",
    "\n",
    "print(len(x_eval[0]))\n",
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6604f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBasedModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def model_builder(self):\n",
    "\n",
    "        ## BERT encoder\n",
    "        encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        encoder.layers[0].trainable = False\n",
    "\n",
    "\n",
    "        ## QA Model\n",
    "        input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "        token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "        attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "        embedding = encoder(\n",
    "            input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "        )[0]\n",
    "\n",
    "        start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
    "        start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "        end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "        end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "        start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "        end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "        model = keras.Model(\n",
    "            inputs=[input_ids, token_type_ids, attention_mask],\n",
    "            outputs=[start_probs, end_probs],\n",
    "        )\n",
    "\n",
    "        \n",
    "       \n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        model.compile(optimizer=optimizer, loss=[loss, loss], run_eagerly=True)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724f745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 12:02:42.436273: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " start_logit (Dense)            (None, 512, 1)       768         ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " end_logit (Dense)              (None, 512, 1)       768         ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['start_logit[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 512)          0           ['end_logit[0][0]']              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 512)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 512)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,776\n",
      "Trainable params: 1,536\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "رشته هوش مصنوعی\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"model_weights.h5\"):\n",
    "    \n",
    "    model_class = BertBasedModel()\n",
    "    model = model_class.model_builder()\n",
    "    model.load_weights('model_weights.h5')\n",
    "    \n",
    "    context = 'من محمد ایزدی هستم. من دانشجوی ارشد رشته هوش مصنوعی هستم.'\n",
    "    question = 'رشته من چیست؟'\n",
    "    start_char_idx = 41\n",
    "    answer_text = 'هوش مصنوعی'\n",
    "    all_answers = []\n",
    "    \n",
    "    squad_eg = TokenizedData(\n",
    "        question, context, start_char_idx, answer_text, all_answers\n",
    "    )\n",
    "    squad_eg.preprocessing()\n",
    "    \n",
    "    x_test, y_test = create_inputs([squad_eg]) \n",
    "    \n",
    "\n",
    "    pred_start, pred_end = model.predict(x_test)\n",
    "    \n",
    "\n",
    "    offsets = squad_eg.context_token_to_char\n",
    "    start = np.argmax(pred_start)\n",
    "    end = np.argmax(pred_end)\n",
    "    if start >= len(offsets):\n",
    "        print('sorry the answer was not good enough :(')\n",
    "        quit()\n",
    "\n",
    "\n",
    "    pred_char_start = offsets[start][0]\n",
    "    if end < len(offsets):\n",
    "        pred_char_end = offsets[end][1]\n",
    "        pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "    else:\n",
    "        pred_ans = squad_eg.context[pred_char_start:]\n",
    "        \n",
    "    print(pred_ans)\n",
    "    with open('answer.txt', 'w') as f:\n",
    "        f.write('متن: '+ context +'\\n')\n",
    "        f.write('جواب صحیح: '+ answer_text +'\\n')\n",
    "        f.write('جواب مدل: '+ pred_ans +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "else:\n",
    "    exact_match_callback = ExactMatch(x_eval, y_eval)\n",
    "    model_class = BertBasedModel()\n",
    "    model = model_class.model_builder()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=3,\n",
    "        verbose=1,\n",
    "        batch_size=8, #این بچ سایز مناسب نیست و صرفا بخاطر اجرا برروی سیسمی ضعیت قرار داده شده است. بهتر از این مقدار را برابر با ۶۴ قرار دهید\n",
    "        callbacks=[exact_match_callback],\n",
    "    )\n",
    "    \n",
    "    model.save_weights('model_weights.h5')\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
